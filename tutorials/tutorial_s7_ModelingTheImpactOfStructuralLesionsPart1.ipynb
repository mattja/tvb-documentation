{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "   INFO  log level set to INFO\n"
     ]
    }
   ],
   "source": [
    "%pylab nbagg\n",
    "from tvb.simulator.lab import *\n",
    "import os\n",
    "import numpy\n",
    "import networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Modeling The Impact of Structural Lesions -- Part I: Modeling Lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore different lesion strategies as presented in [1]. \n",
    "\n",
    "Two main strategies were used:\n",
    "\n",
    "  * sequential single deletion     \n",
    "    * random\n",
    "    * targeted    \n",
    "  * focal\n",
    "\n",
    "The first type, random method,  was aiming to provide a structural failure analysis, that is, to analyse systematically the robustness of the network. The first type, second method, provides an intemediate step toward specific lesioning, by taking into account graph metrics like degree, strength and betweeness centrality to select a target node.  \n",
    "\n",
    "The second type was intended to evaluate functional failure analysis. In this strategy we not only specify a focal node that belongs to a region of the Default Mode Network (DMN) [2, 3] but nodes within a localized extent are deleted as well. \n",
    "The size of the lesions represent 5% of the cortical surface.\n",
    "\n",
    "\n",
    "\n",
    "Here, we will explore what these strategies are about and compute + store some graph metrics of the lesioned connectivities. New matrices won't be saved. The first thing to do is creating an instance of a Connectivity datatype with the connectome we'll use as a control ('healthy').  \n",
    "\n",
    "There are several connectivity matrices available, one of them is the 998 regions originally used in [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  File 'average_orientations' not found in ZIP.\n",
      "WARNING  File 'cortical' not found in ZIP.\n",
      "WARNING  File 'hemispheres' not found in ZIP.\n"
     ]
    }
   ],
   "source": [
    "white_matter = connectivity.Connectivity.from_file(\"connectivity_998.zip\")\n",
    "white_matter.configure()\n",
    "nor = white_matter.number_of_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the properties of this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width=100%><tr><td>Weights - Variance</td><td>0.00900540545862</td></tr><tr><td>Tract lengths - Min. non zero</td><td>4.25</td></tr><tr><td>Tract lengths - Mean</td><td>1.44439569958</td></tr><tr><td>Weights - Maximum</td><td>0.91494284</td></tr><tr><td>Areas - Maximum</td><td>1.5</td></tr><tr><td>Weights - Var. non zero</td><td>0.0100008244541</td></tr><tr><td>Undirected</td><td>0</td></tr><tr><td>Tract lengths - Var. non zero</td><td>1086.4666302</td></tr><tr><td>Tract lengths - Variance</td><td>95.0457103186</td></tr><tr><td>Number of connections</td><td>35730</td></tr><tr><td>Tract lengths - Mean non zero</td><td>40.2637529908</td></tr><tr><td>Areas - Minimum</td><td>1.5</td></tr><tr><td>Tract lengths - Max. non zero (connections)</td><td>189.5</td></tr><tr><td>Areas - Mean</td><td>1.5</td></tr><tr><td>Number of regions</td><td>998</td></tr><tr><td>Weights - Min. non zero</td><td>0.084896185</td></tr><tr><td>Tract lengths - Maximum</td><td>189.5</td></tr><tr><td>Tract lengths - Min. non zero (connections)</td><td>4.25</td></tr><tr><td>Weights - Mean</td><td>0.0179367052556</td></tr><tr><td>Weights - Mean non zero</td><td>0.500000844707</td></tr><tr><td>Tract lengths - Var. non zero (connections)</td><td>1086.4666302</td></tr><tr><td>Tract lengths - Mean non zero (connections)</td><td>40.2637529908</td></tr>"
      ],
      "text/plain": [
       "Connectivity(bound=False, value=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Metrics\n",
    "Graph metrics computed after each node deletion are node degree, strength, global efficiency, largest component and betweeness centrality. \n",
    "\n",
    "We'll import from the `tvb.analyzers.graph` a few useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tvb.analyzers.graph import betweenness_bin, distance_inv, efficiency_bin, get_components_sizes, sequential_random_deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential random deletion \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In orider to be consistent about the lesions, we'll first set the Numpy's random number genrator seed -- this will guarantee that you'll get the same random sequence.\n",
    "\n",
    "A small example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_seed = 42\n",
    "my_random_state = numpy.random.RandomState(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a sequence of random sequence of integer numbers drawn from the regions indices, without replacement, will define the order in which the nodes are going to be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_sequence = my_random_state.choice(numpy.arange(nor), nor-2, replace=False)\n",
    "ns, nd, ge, lc = sequential_random_deletion(white_matter, random_sequence, nor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [1] the graph metrics were computed for 25 different random sequences. So, the next cell contains a loop to do this. It is most certainly not the way to do it since it takes a long long time. This task can/should be parallelized without a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_seeds = [0, 1, 2, 3, 5, 7, 12, 13, 19, 21, 27, 33, 42, 53, 64, 67, 73, 77, 81, 84, 86, 89, 92, 97, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/node_strength_0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9271edc67441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequential_random_deletion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhite_matter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# save them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/node_strength_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_seed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/node_degree_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_seed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/global_efficiency_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_seed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\mw\\Downloads\\TVB_Distribution\\tvb_data\\Lib\\site-packages\\numpy\\lib\\npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/node_strength_0.npy'"
     ]
    }
   ],
   "source": [
    "for this_seed in my_seeds:\n",
    "    my_random_state = numpy.random.RandomState(this_seed)\n",
    "    random_sequence = my_random_state.choice(numpy.arange(nor), nor-2, replace=False)\n",
    "    ns, nd, ge, lc = sequential_random_deletion(white_matter, random_sequence, nor)\n",
    "    # save them\n",
    "    numpy.save('data/node_strength_' + str(this_seed) + '.npy' , ns)\n",
    "    numpy.save('data/node_degree_' + str(this_seed) + '.npy' , nd)\n",
    "    numpy.save('data/global_efficiency_' + str(this_seed) + '.npy' , ge)\n",
    "    numpy.save('data/largest_component_' + str(this_seed) + '.npy' , lc)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential targeted deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ns, nd, nbc, ge, lc = sequential_targeted_deletion(white_matter, nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save them\n",
    "numpy.save('data/node_strength_' + 'ns' + '.npy' , ns)\n",
    "numpy.save('data/node_degree_'   + 'nd' + '.npy' , nd)\n",
    "numpy.save('data/node_betweenness_centrality_'    + 'nbc' + '.npy' , nbc)\n",
    "numpy.save('data/global_efficiency_' + 'ns_nd_bc' + '.npy' , ge)\n",
    "numpy.save('data/largest_component_' + 'ns_nd_bc' + '.npy' , lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the metrics computed for **sequential random deletion**..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_efficiency = numpy.zeros((996, len(my_seeds)))\n",
    "largest_component = numpy.zeros((996, len(my_seeds)))\n",
    "for i, this_seed in enumerate(my_seeds):\n",
    "    # load them\n",
    "    global_efficiency[:, i] = numpy.load('data/global_efficiency_' + str(this_seed) + '.npy')\n",
    "    largest_component[:, i] = numpy.load('data/largest_component_' + str(this_seed) + '.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the ones computed for **sequential targeted deletion** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ge = numpy.load('data/global_efficiency_' + 'ns_nd_bc' + '.npy' )\n",
    "lc = numpy.load('data/largest_component_' + 'ns_nd_bc' + '.npy' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global  efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(20,8))\n",
    "ax.plot(global_efficiency.mean(axis=1), 'k', linewidth=4, label='random')\n",
    "ax.plot(ge[:, 0], 'g', linewidth=4, label='targeted (strength)')\n",
    "ax.plot(ge[:, 1], 'b', linewidth=4, label='targeted (degree)')\n",
    "ax.plot(ge[:, 2], 'r', linewidth=4, label='targeted (centrality)')\n",
    "\n",
    "plt.xlabel('Number of Deleted Nodes', fontsize=24)\n",
    "plt.ylabel('Global Efficiency (binary)', fontsize=24)\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of the largest component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(20,8))\n",
    "ax.plot(largest_component.mean(axis=1), 'k', linewidth=4, label='random')\n",
    "ax.plot(lc[:, 0], 'g', linewidth=4, label='targeted (strength)')\n",
    "ax.plot(lc[:, 1], 'b', linewidth=4, label='targeted (degree)')\n",
    "ax.plot(lc[:, 2], 'r', linewidth=4, label='targeted (centrality)')\n",
    "\n",
    "plt.xlabel('Number of Deleted Nodes',   fontsize=24)\n",
    "plt.ylabel('Size of Largest Component', fontsize=24)\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localized deletions\n",
    "\n",
    "Localized deletions represent area removal, that is, a certain number of nodes and their connections around a focal point were deleted. This strategy was aiming at dynamic and functional failure analysis, since more evident changes in the dynamic patters are expected. \n",
    "\n",
    "The central locations were abribitrarily chosen. In this example we'll only remove node 194 [index 193] and its 49 nearest neighbours (5% of the represented cortical surface). Despite having the fibre distances, the nearest neighbouts were etermined by the Euclidean distance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def localized_deletions(white_matter, lesion_indices, start, end, k=49):\n",
    "    \"\"\"\n",
    "    \n",
    "    Remove a focal node and its k nearest neighbours.\n",
    "    Lesions are constrained to be intrahemispheric.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    white_matter   : a tvb Connectivity datatype    \n",
    "    lesion_indices : array\n",
    "                     an index vector with the focal nodes that will be deleted.\n",
    "                     \n",
    "    k              : int \n",
    "                     neighbours to be deleted.\n",
    "                                        \n",
    "    start          : int \n",
    "                     first index of a given hemisphere\n",
    "                     \n",
    "    end            : int \n",
    "                     last index of a hemisphere\n",
    "                     \n",
    "                     \n",
    "    Notes\n",
    "    -----\n",
    "    eg, first hemipshere start=0; end=nor//2\n",
    "        second hemisphere: start=nor//2; end=nor\n",
    "        \n",
    "        and nor is white_matter.number_of_regions\n",
    "        \n",
    "    .. author:: Paula Sanz Leon\n",
    "    \"\"\"\n",
    "\n",
    "    for lesion in lesion_indices:\n",
    "\n",
    "        # compute distances and such\n",
    "        xo = white_matter.centres[lesion, 0]\n",
    "        yo = white_matter.centres[lesion, 1]\n",
    "        zo = white_matter.centres[lesion, 2]\n",
    "\n",
    "        # only compute distance wrt to intra-hemispheric neighbours\n",
    "        nor = white_matter.number_of_regions\n",
    "        distances = numpy.sqrt((white_matter.centres[start:end, 0] - xo)**2 \n",
    "                    +  (white_matter.centres[start:end, 1] - yo)**2 \n",
    "                    +  (white_matter.centres[start:end, 2] - zo)**2)\n",
    "\n",
    "        # get neighbouring nodes\n",
    "        sorted_euclidean = numpy.argsort(distances)\n",
    "        good_indices     = sorted_euclidean[:k+1]\n",
    "\n",
    "        # lesion\n",
    "        new_connectivity_weights = white_matter.weights.copy()\n",
    "        new_connectivity_weights[good_indices, :] = 0.0\n",
    "        new_connectivity_weights[:, good_indices] = 0.0\n",
    "        \n",
    "        return new_connectivity_weights, good_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lesion_indices           = (numpy.array([194]) - 1).astype(int)\n",
    "lesioned_matrix_weights, idxs  = localized_deletions(white_matter, lesion_indices, 0, nor//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(25,20))\n",
    "\n",
    "# for visualization purposes\n",
    "data = lesioned_matrix_weights.copy()\n",
    "# binarize\n",
    "data[data > 0] = 1\n",
    "data[idxs, :] = 3\n",
    "data[:, idxs] = 3\n",
    "data[lesion_indices, :] = 2\n",
    "data[:, lesion_indices] = 2\n",
    "\n",
    "\n",
    "# make a color map of fixed colors\n",
    "cmap = colors.ListedColormap(['black', 'white', 'red', 'blue'])\n",
    "bounds=[0,1,2,3,4]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "p = ax.pcolormesh(data, cmap=cmap, norm=norm)\n",
    "ax.invert_yaxis()\n",
    "cbar = fig.colorbar(p, cmap=cmap, norm=norm, boundaries=bounds, ticks=[0.5, 1.5, 2.5, 3.5])\n",
    "cbar.ax.set_yticklabels(['no connections', 'connections', 'lesion site', 'neighbours'], fontsize=24)\n",
    "\n",
    "xticks = range(998)\n",
    "yticks = range(998)\n",
    "\n",
    "xticklabels=['L194', ]\n",
    "yticklabels=['L194', ]\n",
    "\n",
    "ax.set_xticks(xticks[193:194])\n",
    "ax.set_xticklabels(xticklabels)\n",
    "\n",
    "ax.set_yticks(yticks[193:194])\n",
    "ax.set_yticklabels(yticklabels)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = white_matter.weights.copy()\n",
    "\n",
    "from matplotlib import colors\n",
    "fig, ax = plt.subplots(1, figsize=(25,20))\n",
    "\n",
    "# for visualization purposes\n",
    "data[data > 0] = 1\n",
    "\n",
    "\n",
    "# make a color map of fixed colors\n",
    "cmap = colors.ListedColormap(['black', 'white'])\n",
    "bounds=[0,1,2]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "p = ax.pcolormesh(data, cmap=cmap, norm=norm)\n",
    "ax.invert_yaxis()\n",
    "cbar = fig.colorbar(p, cmap=cmap, norm=norm, boundaries=bounds, ticks=[0.5, 1.5])\n",
    "cbar.ax.set_yticklabels(['no connections', 'connections', 'lesion site', 'neighbours'], fontsize=24)\n",
    "\n",
    "xticks = range(998)\n",
    "yticks = range(998)\n",
    "\n",
    "xticklabels=['RH']\n",
    "yticklabels=['RH']\n",
    "\n",
    "ax.set_xticks(xticks[998//4:998//4+1])\n",
    "ax.set_xticklabels(xticklabels)\n",
    "\n",
    "ax.set_yticks(yticks[998//4:998//4+1])\n",
    "ax.set_yticklabels(yticklabels)\n",
    "\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) \n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(24) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Alstott et al. (2009) Modeling the impact of lesions in the human brain. Plos Comp Bio.\n",
    "\n",
    "[2] Damoiseaux et al. (2006) Consistent resting-state networks across healthy subjects. PNAS\n",
    "\n",
    "[3] Honey et al. (2009) Predicting human resting-state functional connectivity from structural connectivity. PNAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
